# -*- coding: utf-8 -*-
"""Preface training 20/10

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OxFXGAAaNUI51yw1e90Ghqy9nHfaHQhr
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns 
import numpy as np





loan = pd.read_csv("M3L5/loan_data.csv")
df = pd.read_csv("M3L5/kyphosis.csv")

#GP makes decision
#1. check temp
##2. asks you to cough
#check for sore throats
#stethoscope for checking heart beat
#make his diagnosis -> prescription

#kyphosis -> back is curvature



sns.jointplot(data = df, x = "Start", y = "Number", hue = "Kyphosis")

sns.pairplot(data = df,hue="Kyphosis")

#compare all pairs in this dataset

#step 1 : prepare data
from sklearn.model_selection import train_test_split 
#use this to split data into train and test

#get only Age, Number and Start
X = df.drop("Kyphosis", axis=1) #1 is column, 0 is row
y = df["Kyphosis"]

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=101) 
#random State is to ensure everyone get the same dataset after split

from sklearn.tree import DecisionTreeClassifier

#step 2= select our model
dtree = DecisionTreeClassifier()
dtree

#criterion="gini"

dtree.fit(X_train, y_train)

#visualise the decision tree trained 
from sklearn import tree
tree.plot_tree(dtree) #only works after we trained it with fit

plt.figure(figsize=(16,10))
tree.plot_tree(dtree, feature_names=X.columns,filled="True"); #; and plot.show both way can remove the text above 
plt.show() #feature names is X.columns to subtitute X[0]/X[1].... Filled to show the majority

#1. root node -> top level 
#2. internal node -> inter-level
#3. leaf node -> last level

#value = number of absense, present
#to the left = true, right = false 
#samples = total number of cases

X_train[X_train["Number"]<=4.5].shape

#gini: gini impurity = how crucial is a particular condition in making a decision 
# gini = the lower the better, directly links to the true or false on the first branch
#the node is decided by the lowest gini across other combinations
#human logic, more important factor, place it first

#step 4: prediction

dtree_pref = dtree.fit(X_test, y_test)

dtree_pred=dtree.predict(X_test) #using the trained model to predict with X testing data
dtree_pred

#step 5: evaluation
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

accuracy_score(y_test, dtree_pred) #check the accuracy with the answer (Y_test)

confusion_matrix(y_test, dtree_pred) #???

print(classification_report(y_test,dtree_pred))

#Decision tree
#pros: easy to understand, simple to use
#cons: inaccurate, root node plays a huge factor in deicison making

#random forest 
#capture simplicity of decision trees, avoid the biasness in the root node

#step 2: model selection 
from sklearn.ensemble import RandomForestClassifier

#step 3: instantiate

rfc = RandomForestClassifier()
dtree = DecisionTreeClassifier()

dtree

rfc = RandomForestClassifier(n_estimators=200)
rfc
#Max_features="auto" = square root of no. of features +- 10% (16-> 4,5,6)
#bootstrap = True --> bootstrapped dataset
#n_estimators = 100 (100 trees)
#criterion ="gini" --> we still use to decide out of all random features, which is more important

rfc.fit(X_train,y_train)

rfc.estimators_[0] #check the number of trees

#visualise decision tree in random forest tree ensemble

plt.figure(figsize=(16,10))
tree.plot_tree(rfc.estimators_[150],filled=True,feature_names=X.columns);

#boostrap + Aggregation = bagging
#bag 1/3 of the records for testing
#bootstrap: make decision trees by selecting 2 features, repeat process for the estimated trees

#aggregation: aggregate the results and get the more popular result
#used the bagged record to test the model

#predict 
rf_pred=rfc.predict(X_test)
rf_pred

accuracy_score(y_test,rf_pred)

confusion_matrix(y_test,rf_pred)

print(classification_report(y_test,rf_pred)) #what's recall